{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ShakesPeare Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbCidSM_bCTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d2b7d55b-a134-4f2d-b07a-a6c9e7fc841d"
      },
      "source": [
        "import os\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "path_2_file = tf.keras.utils.get_file('shakespeare.txt',\n",
        "                                      'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHBNsgexlBx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "14a3109f-2709-4bdf-af31-3aaabbe17392"
      },
      "source": [
        "text = open(path_2_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(text[:250])\n",
        "vocab = sorted(set(text))\n",
        "print(\"{} unique characters\".format(len(vocab)))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e5qNHjynOfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "2841975e-23a8-447a-d3f3-eb05e090d3de"
      },
      "source": [
        "char2int = {unique:intg for intg, unique in enumerate(vocab)}\n",
        "int2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2int[char] for char in text])\n",
        "print(\"{\")\n",
        "for char,_ in zip(char2int, range(20)):\n",
        "  print(\"    {:4s}: {:3d},\".format(repr(char), char2int[char]))\n",
        "\n",
        "print('...\\n')\n",
        "print('{} ----> characters append to int ----> {}'.format(repr(text[:13]),\n",
        "                                                          text_as_int[:13]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    '\\n':   0,\n",
            "    ' ' :   1,\n",
            "    '!' :   2,\n",
            "    '$' :   3,\n",
            "    '&' :   4,\n",
            "    \"'\" :   5,\n",
            "    ',' :   6,\n",
            "    '-' :   7,\n",
            "    '.' :   8,\n",
            "    '3' :   9,\n",
            "    ':' :  10,\n",
            "    ';' :  11,\n",
            "    '?' :  12,\n",
            "    'A' :  13,\n",
            "    'B' :  14,\n",
            "    'C' :  15,\n",
            "    'D' :  16,\n",
            "    'E' :  17,\n",
            "    'F' :  18,\n",
            "    'G' :  19,\n",
            "...\n",
            "\n",
            "'First Citizen' ----> characters append to int ----> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5MwjWS0oRcv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c6fae27c-b9f3-423e-ab45-c22f51f24b1b"
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "for i in char_dataset.take(5):\n",
        "  print(int2char[i.numpy()])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LauImVSIqa_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "83e2d35a-9db5-4e86-add1-12766f313a4c"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length +1)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for item in sequences.take(5):\n",
        "  print(repr(\"\".join(int2char[item.numpy()])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHNW6OMfq_FW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "67cc724a-0be7-4a98-d27b-7a80e1934c20"
      },
      "source": [
        "def split_input(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input)\n",
        "\n",
        "for input_eg, target_eg in dataset.take(1):\n",
        "  print(\"input data\", repr(''.join(int2char[input_eg.numpy()])))\n",
        "  print(\"target data\", repr(''.join(int2char[target_eg.numpy()])))\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input data 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "target data 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X8jAK2HsiVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "8194691c-8c31-45aa-be6d-76c09fbe6662"
      },
      "source": [
        "for i, (input_int, target_int) in enumerate(zip(input_eg[:5],\n",
        "                                            target_eg[:5])):\n",
        "  print('step {:4d}'.format(i))\n",
        "  print('  input {} ({:s})'.format(input_int, repr(int2char[input_int])))\n",
        "  print('expected output {} ({:s})'.format(target_int, repr(int2char[target_int])))\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step    0\n",
            "  input 18 ('F')\n",
            "expected output 47 ('i')\n",
            "step    1\n",
            "  input 47 ('i')\n",
            "expected output 56 ('r')\n",
            "step    2\n",
            "  input 56 ('r')\n",
            "expected output 57 ('s')\n",
            "step    3\n",
            "  input 57 ('s')\n",
            "expected output 58 ('t')\n",
            "step    4\n",
            "  input 58 ('t')\n",
            "expected output 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxuHUOgCuPAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2508617d-a55c-4fe1-fd66-87ee5f4bc08a"
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(\n",
        "                                   vocab_size, embedding_dim,\n",
        "                                   batch_input_shape=[batch_size,None]),\n",
        "                               tf.keras.layers.GRU(rnn_units, return_sequences=True,\n",
        "                                                   stateful=True,\n",
        "                                                   recurrent_initializer='glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "model = build_model(vocab_size = len(vocab), \n",
        "                    embedding_dim = embedding_dim,\n",
        "                    rnn_units= rnn_units, \n",
        "                    batch_size= batch_size\n",
        "                    )\n",
        "for input_eg_batch, target_eg_batch in dataset.take(1):\n",
        "  eg_batch_pred = model(input_eg_batch)\n",
        "  print(eg_batch_pred.shape)\n",
        "model.summary()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmSgIHfoyLnJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "26130af4-cfa7-4406-8465-3b47c0607ed7"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels,\n",
        "                                                         logits, from_logits=True)\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'chkpr_{epoch}')\n",
        "checkpoint_call = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only = True\n",
        ")\n",
        "epochs = 25\n",
        "history = model.fit(dataset, epochs = epochs, callbacks = [checkpoint_call])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 172 steps\n",
            "Epoch 1/25\n",
            "172/172 [==============================] - 13s 75ms/step - loss: 2.6785\n",
            "Epoch 2/25\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.9739\n",
            "Epoch 3/25\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.7024\n",
            "Epoch 4/25\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.5508\n",
            "Epoch 5/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.4607\n",
            "Epoch 6/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.4004\n",
            "Epoch 7/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.3529\n",
            "Epoch 8/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.3150\n",
            "Epoch 9/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.2795\n",
            "Epoch 10/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.2481\n",
            "Epoch 11/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.2152\n",
            "Epoch 12/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.1844\n",
            "Epoch 13/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.1521\n",
            "Epoch 14/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.1190\n",
            "Epoch 15/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.0850\n",
            "Epoch 16/25\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 1.0500\n",
            "Epoch 17/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.0155\n",
            "Epoch 18/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 0.9796\n",
            "Epoch 19/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 0.9436\n",
            "Epoch 20/25\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 0.9093\n",
            "Epoch 21/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 0.8783\n",
            "Epoch 22/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 0.8474\n",
            "Epoch 23/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 0.8212\n",
            "Epoch 24/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 0.7946\n",
            "Epoch 25/25\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 0.7729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIgtSNh3kCIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24e45dd5-ea25-4f8d-c209-086815162820"
      },
      "source": [
        "example_batch_loss  = loss(target_eg_batch, eg_batch_pred)\n",
        "print(\"Prediction shape: \", input_eg_batch.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", eg_batch_pred.numpy().mean())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       0.00038520095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYrKu153lbDB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c1989e6-c2b2-4fae-e131-91ab324a441e"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  generate_num = 10000\n",
        "  input_eval = [char2int[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval,0)\n",
        "\n",
        "  text_generated = []\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(generate_num):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions,0)\n",
        "\n",
        "    predictions = predictions/ temperature\n",
        "    predict_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval = tf.expand_dims([predict_id],0)\n",
        "\n",
        "    text_generated.append(int2char[predict_id])\n",
        "\n",
        "  return(start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string='Romeo: '))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Romeo: he is gone here in safety\n",
            "Of boans but well, and what the humour wounds\n",
            "With his reporting.\n",
            "\n",
            "COMINIUS:\n",
            "He's simple at the legs.\n",
            "\n",
            "BLUNT:\n",
            "Hargin, commend me to: the twice stand to see.\n",
            "\n",
            "KING RICHARD III:\n",
            "Servant, sir, a gentlemannoryork and beauty sound,\n",
            "A thousand tybants; for I have heard to do\n",
            "Wilt thou had?\n",
            "\n",
            "POMPEY:\n",
            "Sir, so shame too true: and to't o' the moon!\n",
            "\n",
            "KATHARINA:\n",
            "Your bound, methinks I know they are at your bear.\n",
            "\n",
            "KATHARINA:\n",
            "In father for the loss of thatlow shakest thou there?\n",
            "\n",
            "ESCALUS:\n",
            "What very grass to have good pursues to the public war\n",
            "That buried weeds enough:\n",
            "My gradity take my dew so up man: his request?\n",
            "As I have no dangerous to the back\n",
            "Of that sixteen gentleman, thoughts me aroul\n",
            "In all the rack again, or take remembrance or crave?\n",
            "\n",
            "GRUMIO:\n",
            "Here, sir: you must take my head; and that's the cur the appleant\n",
            "Of benefactorless reason, that wilt thou?\n",
            "\n",
            "Boatswain:\n",
            "Where is the prince's death!\n",
            "a tabour to-night!\n",
            "\n",
            "RICHMOND:\n",
            "Some say not Romeo n thy too cruel senators\n",
            "To be thus fling their harm to sin.\n",
            "One pitch thou please to look on him, who\n",
            "excuses with ill that foul pressmuny\n",
            "What is left of the welcome.\n",
            "\n",
            "CAMILLO:\n",
            "Hang death or doing them?\n",
            "\n",
            "VALERIA:\n",
            "In traitor, I bring thee not toain softens me or light comfort.\n",
            "\n",
            "ALONSO:\n",
            "Thanks, being a better hand.\n",
            "\n",
            "LADY ANNE:\n",
            "And look upon him, what says my life,\n",
            "Provoked by him. My words could speak from me,\n",
            "'Tis time what reacheth times to-night\n",
            "Ere subdue thee more to be as even to\n",
            "cordual thing.\n",
            "\n",
            "First Gendleman:\n",
            "I told you not what chather Conjurate and eyes\n",
            "Of twice our reboar se, hath made a whip\n",
            "Of good good particulates and myself,\n",
            "My man's mistress, march my sovereign live;\n",
            "But, as you break of blows of this bishops'd before\n",
            "I may rise and 'twas thy deth our solemn and attaight at\n",
            "you did! here detch you well.\n",
            "\n",
            "Servant:\n",
            "My lord, here comes the ned prince?\n",
            "\n",
            "KATHARINA:\n",
            "Good fools! O thousand times correction, and eat\n",
            "Salther in the very part ere my wife;\n",
            "Your mistress' noble horse;\n",
            "Then, wife, nor English sorrow with Cominius\n",
            "We have stumphip he came with her to his shoulds:\n",
            "y.\n",
            "We are all men's; therefore I'll write it, lords?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "And to me, then minister i':\n",
            "Now must the way; for there, as thou liest'\n",
            "O think, and saveth Romans Lucentio.\n",
            "No, no;\n",
            "I am out my brother so be you.\n",
            "\n",
            "RICHARD:\n",
            "A king, other wronged sir,\n",
            "To bair my fellow-brook Richard; thou dost fear\n",
            "What offect his ouffiction: should you pluck\n",
            "SecondsK did I love my daughter.\n",
            "Good, shield thou dead; 'tis in my midst or execute.\n",
            "\n",
            "ANTONIO:\n",
            "Think'st up retires to-marriage.\n",
            "\n",
            "HORTENSIO:\n",
            "How will my master sensible, O, how much is the sentence.\n",
            "\n",
            "BAPTISTA:\n",
            "Away, my heart!\n",
            "\n",
            "LADY CAPULET:\n",
            "Tall you, in faith, which I find Charion so long's haste.\n",
            "Come, where's my daughter? will't is at thee\n",
            "To grant such less o' the holy mot\n",
            "That box ibstructions makes thee before.\n",
            "\n",
            "BRUTUS:\n",
            "They have met, were not the time\n",
            "Whose voice hath stopp'd my love's true.\n",
            "Poor knave to bitter loyal,\n",
            "Near to the Duke of Gloucester's.\n",
            "Good Stiff, master, is they were convey'd to mose\n",
            "in quarrel of the Lady Bona tard you for your king;\n",
            "I make my daughter.\n",
            "\n",
            "GLOUCESTER:\n",
            "Why, how often himself? where is Alway, would they last\n",
            "Upon thy word away with Abroad,\n",
            "Bequeath, and breaks, to wear a country.\n",
            "\n",
            "ANTONIO:\n",
            "Well, be, that you are well met: sir, hearth and lowling bride.\n",
            "\n",
            "POLIXENES:\n",
            "'Tis well; for that of that herd have eat.\n",
            "\n",
            "CLAUDIO:\n",
            "Come, who's the noble child?\n",
            "O holy where is Lucentio, then I saw,\n",
            "And bring the watch is like a lawful blood,\n",
            "Come on, howliste, and was it first?\n",
            "\n",
            "TRANIO:\n",
            "'Tis passed out an unstance:'tis as before\n",
            "I make our foots, my Lord Northumberland.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Then speak but my freedom,\n",
            "For my difference for'd to Rome,\n",
            "What place we so? find for you:\n",
            "you will, sir; and come anon it muffounds;\n",
            "Or say, 'going their functions: they do show them to\n",
            "Whose first in power and be hunt again,\n",
            "And in possession of the very essage, when he have been\n",
            "man and honour, I will lead forged to any plant;\n",
            "In that would send to him and he that hath kinsman in their lives: our soft calouing\n",
            "Torrow on thy bisth, the times degree\n",
            "And stay deliver thee in the lett, in the name of Rome,\n",
            "no more best, ours, or to be rough, some but a\n",
            "golden men?\n",
            "Would you will hear me of you to have strucked as. Your better, time-pledge of his. Canst thou happe:\n",
            "I was too hard we kill through the cover'd humility.\n",
            "\n",
            "VOLUMNIA:\n",
            "Speak, good soul, sound; ha!\n",
            "\n",
            "POMPEY:\n",
            "Does cloud my slave?\n",
            "\n",
            "Phies that he\n",
            "would you take the good hate of my lambs\n",
            "As cusious a thing that makes your bodies\n",
            "Would be thus bold poison, to county right wakes your fellows,\n",
            "and thou dismonher'd ingering to the vision of but by the store,\n",
            "England, all tenderendst quo light in madam.\n",
            "\n",
            "HASTINGS:\n",
            "I thank thee, darken'd, and blessedly gods!\n",
            "And if he then aboar her, since we are\n",
            "Alat all that shines unextranch more sever's rumour\n",
            "Dropmiss--\n",
            "\n",
            "BALTHASAR:\n",
            "I dreamt my lady to the a several hours:\n",
            "Suppose that there. For his heart be, love; for, appear loud,\n",
            "That, llad me for that, to appear before to be\n",
            "unmorning: still the King of France.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "O, come at first, for Stin\n",
            "Our solest villain, do not so,\n",
            "You are again; but you are moves, Petruchio,' I mean our tatter:\n",
            "The bridegroom for some fresh-frown through upon you\n",
            "With them have I offer'd her and ranch'd with her,\n",
            "Depending herds, to Juliet, and that I am,\n",
            "For I as for meing, of her eyes\n",
            "Of mirth, my without, and my brother,\n",
            "And our tyranntion! know you make remembered,\n",
            "Drabs creeper of great appointments,\n",
            "In hairs must not from us all, whose said 'Rong again.\n",
            "\n",
            "CORIOLANUS:\n",
            "My loving that, I have no courteous children\n",
            "Of my Duke of Gloucester, were you were congeal?\n",
            "\n",
            "GREMIO:\n",
            "Then be not so raised, both beyou this knave below;\n",
            "Or stunding from my love and skill of wretched wreck:\n",
            "This day should stand upon her lip; why should we feem\n",
            "The castle, when he wakes: some day of the eye,\n",
            "bestrive their familiar say'st, true; good my lord,\n",
            "And left the loss of January take at once.\n",
            "\n",
            "KING HENRY VI:\n",
            "For what, I do?\n",
            "\n",
            "Second Senator:\n",
            "You are coming dumpst?\n",
            "\n",
            "SICINIUS:\n",
            "This is well acquainted helmath bring\n",
            "And loss of sails and painted by thy blame\n",
            "As more healthy and libely on their accuses, whom here\n",
            "Who rather feel the path-more most sweetly, speak.\n",
            "\n",
            "CAPULET:\n",
            "An's thought!--I must confess I,\n",
            "'Simity, we were as boldnoon's by somewhat work\n",
            "When he may deliver you o' the slaughter\n",
            "A name invetties our bosom; but if we will,\n",
            "To high proportion keas my fan advertised\n",
            "Then, since that they say 'Al.\n",
            "\n",
            "KING HENRY VI:\n",
            "Far be the moon, they are inclined\n",
            "Would cap of cheets, or live armiss thy cloudy.\n",
            "\n",
            "RIVERS:\n",
            "So she you we, to win awhile.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Bring me this churchyard ts;\n",
            "This day myself above the heart of honour\n",
            "These two and three-time of thy weapon, they are\n",
            "great alive, the Earl of Wiltshire? why,\n",
            "Who are beasts for thee to noble peace the hardy mead,\n",
            "That friar, that tial dull against its fair,\n",
            "Out of our souls, their freedful ignorance\n",
            "With old hearts Englishead, not too much.\n",
            "\n",
            "CALIBAN:\n",
            "O, pardon, poor Doricious villain,\n",
            "And you bear me stays a Verona, saveis.\n",
            "\n",
            "HERMIONE:\n",
            "Would they were under the sway?\n",
            "We bring the worst. Whatscand the villain\n",
            "That your father very each other the\n",
            "earth to make me but the first word of the Lord.\n",
            "Your degree to his chy of these was\n",
            "As bright aspeality?\n",
            "\n",
            "ANGELO:\n",
            "With all planets of this sweet? O rude, O, word lord!\n",
            "I should to God all at Verona?\n",
            "\n",
            "PAULINA:\n",
            "Some meet, grace to be unto thee.\n",
            "\n",
            "VIRGILIA:\n",
            "I cannot speak what nord. Is the wars did fast?\n",
            "\n",
            "Second Mourge\n",
            "Tell them that which they were power: but, whilst I\n",
            "live, by that against our maprication\n",
            "Ere husband to hear these forest bent\n",
            "Hix conclury.\n",
            "\n",
            "CAMILLO:\n",
            "Yes; and whom I would nor destity too,\n",
            "He stinces here lies such necessity they sunden;\n",
            "Who am sovereign, with nothing but cracking of the city,\n",
            "And make shooping things give conscience;\n",
            "O, pardoning; here thou brook'd the near to?\n",
            "\n",
            "TRANIO:\n",
            "Have we round,\n",
            "We must seek thee here to dream out.\n",
            "\n",
            "AUTOLYCUS:\n",
            "If that the stay:\n",
            "As careless loss, horse! and hasto here die.\n",
            "\n",
            "RATCLIFF:\n",
            "Come, how this bigger, being taken; and first begg!\n",
            "\n",
            "DERBY:\n",
            "I have not jead, daughter, fetch him out of Mars's appear,\n",
            "Amours the severity and honour.\n",
            "Mark 'tward, and gentleman, on pain of batter;\n",
            "no honour and woman before I men's ease.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Those care the hate;\n",
            "And buried--Cast mock'd drawn soon portures to my Ragozine,\n",
            "Having their bights watch;\n",
            "A new gentleman that true being\n",
            "villany sir Polixenes,\n",
            "Sickness from his short banish'd his commissions.\n",
            "\n",
            "ROMEO:\n",
            "Thou'rt both to be my trath, save you; but you are\n",
            "must I call her? for he devils, their fathers' gall\n",
            "And credution as\n",
            "we hear any nothing.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Art thou! foot my son, I cannot so!\n",
            "\n",
            "SUMIRANUS:\n",
            "Fare you well, good father;\n",
            "Not love with me: 'tis over than death\n",
            "Be traitor to yours.\n",
            "\n",
            "LADY ANNE:\n",
            "What fatal straight shall lord?\n",
            "\n",
            "LADY ANNE:\n",
            "I will; and, so we shall find their clouds behavicre,\n",
            "That I may move it.\n",
            "\n",
            "GLOUCESTER:\n",
            "Poor queen, mothers; gave lost,\n",
            "And schoolmman not been usive with an\n",
            "your beave nor purpuie Baptista's earth.\n",
            "\n",
            "First Sold!\n",
            "\n",
            "KING HENRY VI:\n",
            "Be not so rie held not to be here well in such a dapleh; here's a fall,\n",
            "To mildish reasons with my valitial:\n",
            "if that trust me with the canker, but ome that tirrs back thee.\n",
            "Have three meremembrated in it, and with more sun\n",
            "Thou people the case, Pompey, thou lay'st, these wounds so well: hercues that rage,\n",
            "Lucentio's farm or, and thy words and looks.\n",
            "Our shows about the accusation. I have spoke with me,\n",
            "And chosel not our to--\n",
            "\n",
            "GLOUCESTER:\n",
            "Have benefit you?\n",
            "\n",
            "MARCIUS:\n",
            "O, spare my lungs\n",
            "cank to but right whence sad my queen's able to expose our land\n",
            "After hand, when we came ERIZABETH:\n",
            "What kill'd my neglant, be good and bring it then.\n",
            "My fairer divine, and my lord's sons, which I would spray\n",
            "\n",
            "BUSHY:\n",
            "All hail, fair love! do not some hit to his charge:\n",
            "What answer six unsuance strain'd for stranger:\n",
            "Myself Romeo, take my colour'd, be content.\n",
            "\n",
            "KATHARINA:\n",
            "I would, she hath no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrYUDzSCn8Jb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}